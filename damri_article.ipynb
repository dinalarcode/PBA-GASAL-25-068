{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa981313",
   "metadata": {},
   "source": [
    "Proyek ini bertujuan untuk melakukan text summarization otomatis terhadap berita-berita mengenai Perum DAMRI dari berbagai sumber daring.\n",
    "Proses meliputi tahapan scraping data, preprocessing, embedding berbasis MiniLM, serta pembuatan ringkasan (summary) dengan dua pendekatan:\n",
    "\n",
    "* Semantic Similarity (MiniLM)\n",
    "* TF-IDF Extractive Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7475bef",
   "metadata": {},
   "source": [
    "# Data Preparation -> Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356b29dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install trafilatura"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaad54a5",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49985d6a",
   "metadata": {},
   "source": [
    "### Data Loading dan Persiapan Awal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9f0e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from time import sleep\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d5160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_csv = 'data/damri_article/data_raw/Kelompok1_Link Berita_DAMRI - Data.csv'\n",
    "output_csv = 'data/damri_article/data_processed/scraped_articles.csv'\n",
    "\n",
    "df_urls = pd.read_csv(input_csv)\n",
    "urls = df_urls['link'].dropna().astype(str).tolist()\n",
    "sources = df_urls['sumber'].dropna().astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1ee21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_text(html_text):\n",
    "    text = BeautifulSoup(html_text, \"html.parser\").get_text(separator=\" \")\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'Cookies|Setuju|Kebijakan Privasi|Iklan|Advertisement|ADVERTISEMENT|Copyright', '', text, flags=re.IGNORECASE)\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a39edfc",
   "metadata": {},
   "source": [
    "### Web Scraping Berita Menggunakan Trafilatura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5469013d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import trafilatura\n",
    "\n",
    "def scrape_content(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": (\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/120.0.0.0 Safari/537.36\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            return {\n",
    "                \"url\": url,\n",
    "                \"title\": None,\n",
    "                \"content\": None,\n",
    "                \"error\": f\"Status code: {response.status_code}\"\n",
    "            }\n",
    "            \n",
    "        downloaded = trafilatura.extract(\n",
    "            response.text,\n",
    "            include_comments=False,\n",
    "            include_links=False,\n",
    "            include_tables=False,\n",
    "            deduplicate=True,\n",
    "        )\n",
    "\n",
    "        if not downloaded or not downloaded.strip():\n",
    "            return {\n",
    "                \"url\": url,\n",
    "                \"title\": None,\n",
    "                \"content\": None,\n",
    "                \"error\": \"Konten kosong\"\n",
    "            }\n",
    "\n",
    "        metadata = trafilatura.extract_metadata(response.text)\n",
    "        title = metadata.title if metadata and metadata.title else None\n",
    "\n",
    "        return {\n",
    "            \"url\": url,\n",
    "            \"title\": title,\n",
    "            \"content\": downloaded.strip(),\n",
    "            \"error\": None\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"url\": url,\n",
    "            \"title\": None,\n",
    "            \"content\": None,\n",
    "            \"error\": str(e)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6830f027",
   "metadata": {},
   "source": [
    "### Proses Scraping Secara Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cd9681",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i, (url, sumber) in enumerate(zip(urls, sources), start=1):\n",
    "    print(f\"[{i}/{len(urls)}] Scraping: {url}\")\n",
    "    data = scrape_content(url)\n",
    "    data['sumber'] = sumber\n",
    "    results.append(data)\n",
    "    sleep(random.uniform(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25171c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame(results)\n",
    "df_result.to_csv(output_csv, index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0042c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Scraping selesai! Hasil disimpan di: {output_csv}\")\n",
    "print(f\"Total artikel berhasil: {df_result['content'].notna().sum()} dari {len(df_result)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a46618e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ok = df_result.loc[df_result[\"error\"].isna()]\n",
    "df_ok[\"sumber\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb1ca6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_error = df_result.loc[df_result[\"error\"].notna()]\n",
    "df_error[\"sumber\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425c6c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ok = df_result.loc[df_result[\"error\"].isna()].copy()\n",
    "df_ok[\"content_cleaned\"] = df_ok[\"content\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10febc49",
   "metadata": {},
   "source": [
    "### Pembersihan dan Tokenisasi Teks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3e6a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "df_ok = df_ok.copy()\n",
    "\n",
    "df_ok[\"content_cleaned_tokenize\"] = df_ok[\"content_cleaned\"].apply(\n",
    "    lambda text: sent_tokenize(text) if isinstance(text, str) else []\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7aa4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ok.content_cleaned_tokenize[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77695b50",
   "metadata": {},
   "source": [
    "## Semantic Embedding dengan MiniLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b429942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "df_ok = df_ok.copy()\n",
    "df_ok[\"text_for_embedding\"] = df_ok[\"content_cleaned_tokenize\"].apply(\n",
    "    lambda sentences: \" \".join(sentences) if isinstance(sentences, list) else str(sentences)\n",
    ")\n",
    "\n",
    "texts = df_ok[\"text_for_embedding\"].tolist()\n",
    "embeddings = model.encode(texts, convert_to_tensor=True, batch_size=16, show_progress_bar=True)\n",
    "\n",
    "df_ok[\"embeddings\"] = [emb for emb in embeddings]\n",
    "df_ok[[\"sumber\", \"title\", \"embeddings\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6073343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sample_sentences = df_ok.iloc[0][\"content_cleaned_tokenize\"]\n",
    "sentence_embeddings = model.encode(sample_sentences, convert_to_tensor=True)\n",
    "print(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ad2fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_matrix = cosine_similarity(sentence_embeddings.cpu().numpy())\n",
    "print(similarity_matrix)\n",
    "\n",
    "sentence_scores = similarity_matrix.mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae61e915",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 3\n",
    "top_sentence_indices = np.argsort(sentence_scores)[-top_n:]\n",
    "\n",
    "top_sentence_indices.sort()\n",
    "\n",
    "summary = \" \".join([sample_sentences[i] for i in top_sentence_indices])\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6f0e60",
   "metadata": {},
   "source": [
    "## Extractive Summarization berbasis Semantic Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eaf9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(sentences, model, top_n=3):\n",
    "    if not sentences:\n",
    "        return \"\"\n",
    "    embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "    sim = cosine_similarity(embeddings.cpu().numpy())\n",
    "    scores = sim.mean(axis=1)\n",
    "    top_idx = np.argsort(scores)[-top_n:]\n",
    "    top_idx.sort()\n",
    "    return \" \".join([sentences[i] for i in top_idx])\n",
    "\n",
    "df_ok[\"summary\"] = df_ok[\"content_cleaned_tokenize\"].apply(\n",
    "    lambda sents: summarize_text(sents, model, top_n=3)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c058ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ok.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdc8573",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ok[[\"url\", \"title\", \"content\", \"summary\"]].to_csv(\"data/damri_article/data_processed/data_extraction_minilm_summary.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b294489",
   "metadata": {},
   "source": [
    "## TF-IDF Based Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4993c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "def tfidf_summarize(text, num_sentences=3):\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    text = clean_text(text)\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    if len(sentences) == 0:\n",
    "        return \"\"\n",
    "    if len(sentences) <= num_sentences:\n",
    "        return \" \".join(sentences)\n",
    "\n",
    "    clean_sentences = [\n",
    "        re.sub(r'\\s+', ' ', re.sub(r'[^a-zA-Z0-9.,!? ]', '', s)).strip()\n",
    "        for s in sentences\n",
    "    ]\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(clean_sentences)\n",
    "    sentence_scores = tfidf_matrix.sum(axis=1).A1\n",
    "\n",
    "    top_indices = np.argsort(sentence_scores)[-num_sentences:]\n",
    "    top_indices.sort()\n",
    "\n",
    "    summary = \" \".join([sentences[i] for i in top_indices])\n",
    "    summary = re.sub(r'\\s+', ' ', summary).strip()\n",
    "    return summary\n",
    "\n",
    "df_ok[\"tf_idf_summary\"] = df_ok[\"content\"].apply(lambda x: tfidf_summarize(x, num_sentences=3))\n",
    "\n",
    "df_ok[[\"url\", \"title\", \"tf_idf_summary\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06698458",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ok[[\"url\", \"title\", \"content\", \"tf_idf_summary\"]].to_csv(\"data\\damri_article\\data_processed\\data_extraction_minilm_summary.csv\", sep=\";\")\n",
    "df_ok.to_csv(\"data\\damri_article\\data_processed\\data_extraction_summary[full].csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f0f047",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_ok.summary[0])\n",
    "print(df_ok.tf_idf_summary[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0e34a8",
   "metadata": {},
   "source": [
    "# Sentiment Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e385d82e",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2e44e0",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6dd79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_guess = \"data\\damri_article\\data_processed\\data_extraction_summary[full].csv\"\n",
    "df = pd.read_csv(path_guess, delimiter=\";\", engine=\"python\", on_bad_lines=\"skip\")\n",
    "\n",
    "df['summary'] = df['summary'].astype(str).str.strip()\n",
    "print('Dataset loaded. Rows:', len(df))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9f4078",
   "metadata": {},
   "source": [
    "### Labeling (Keyword-base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7304321c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_keywords = {\n",
    "    \"jadwal\": [\"jadwal\", \"berangkat\", \"keberangkatan\", \"waktu\", \"operasional\", \"pukul\", \"beroperasi\"],\n",
    "    \"rute\": [\"rute\", \"trayek\", \"jalur\", \"pemberhentian\", \"melayani\", \"ke\"],\n",
    "    \"harga\": [\"harga\", \"tiket\", \"tarif\", \"biaya\", \"rp\", \"promo\", \"diskon\", \"gratis\"],\n",
    "    \"layanan\": [\"layanan\", \"fasilitas\", \"kenyamanan\", \"armada\", \"pembayaran\", \"aplikasi\", \"pelayanan\"],\n",
    "    \"umum\": [\"damri\", \"mobilisasi\", \"masyarakat\", \"program\", \"dukungan\", \"pengumuman\"]\n",
    "}\n",
    "\n",
    "def label_aspek_informasi(text):\n",
    "    t = str(text).lower()\n",
    "    scores = {k:0 for k in label_keywords}\n",
    "    for k, kws in label_keywords.items():\n",
    "        for kw in kws:\n",
    "            if kw in t:\n",
    "                scores[k] += 1\n",
    "    best = max(scores, key=lambda x: scores[x])\n",
    "    return best if scores[best] > 0 else \"umum\"\n",
    "\n",
    "df['aspek_informasi_auto'] = df['summary'].apply(label_aspek_informasi)\n",
    "print(df['aspek_informasi_auto'].value_counts())\n",
    "\n",
    "# Save for manual checking\n",
    "df.to_csv(\"data/damri_article/data_processed/data_extraction_labeled_auto.csv\", index=False)\n",
    "print(\"Saved auto-labeled CSV to /mnt/data/data_extraction_labeled_auto.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70072ce",
   "metadata": {},
   "source": [
    "### Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06859a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "review_path = \"data/damri_article/data_processed/data_extraction_labeled_reviewed.csv\"\n",
    "if os.path.exists(review_path):\n",
    "    df = pd.read_csv(review_path, engine=\"python\", on_bad_lines=\"skip\")\n",
    "    print(\"Loaded reviewed labels.\")\n",
    "else:\n",
    "    print(\"No reviewed file found; using auto labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc679e40",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f295ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "syn_lex = {\n",
    "    \"harga\": [\"tarif\",\"biaya\",\"ongkos\"],\n",
    "    \"tiket\": [\"karcis\",\"tiket perjalanan\"],\n",
    "    \"berangkat\": [\"berlepas\",\"mulai berangkat\"],\n",
    "    \"layanan\": [\"pelayanan\",\"service\"],\n",
    "    \"fasilitas\": [\"sarana\",\"prasarana\"],\n",
    "    \"armada\": [\"kendaraan\",\"bus\"],\n",
    "    \"gratis\": [\"bebas biaya\"],\n",
    "    \"rute\": [\"jalur\",\"trayek\"]\n",
    "}\n",
    "\n",
    "def synonym_replace(text, n_repl=1):\n",
    "    words = text.split()\n",
    "    if not words: return text\n",
    "    idxs = list(range(len(words)))\n",
    "    random.shuffle(idxs)\n",
    "    for i in idxs:\n",
    "        w = words[i].lower().strip('.,;:?!')\n",
    "        if w in syn_lex:\n",
    "            words[i] = random.choice(syn_lex[w])\n",
    "            break\n",
    "    return \" \".join(words)\n",
    "\n",
    "def random_deletion(text, p=0.1):\n",
    "    words = text.split()\n",
    "    new_words = [w for w in words if random.random() > p]\n",
    "    return \" \".join(new_words) if new_words else text\n",
    "\n",
    "def random_swap(text):\n",
    "    words = text.split()\n",
    "    if len(words) < 2: return text\n",
    "    i, j = random.sample(range(len(words)), 2)\n",
    "    words[i], words[j] = words[j], words[i]\n",
    "    return \" \".join(words)\n",
    "\n",
    "def augment_text(text):\n",
    "    return [synonym_replace(text), random_deletion(text), random_swap(text)]\n",
    "\n",
    "# Build augmented dataset\n",
    "min_samples_per_class = 20\n",
    "label_col = 'aspek_informasi_auto'\n",
    "aug_rows = []\n",
    "\n",
    "for label, group in df.groupby(label_col):\n",
    "    count = len(group)\n",
    "    if count >= min_samples_per_class:\n",
    "        continue\n",
    "    needed = min_samples_per_class - count\n",
    "    samples = group['summary'].tolist()\n",
    "    i = 0\n",
    "    while needed > 0:\n",
    "        src = samples[i % len(samples)]\n",
    "        for aug in augment_text(src):\n",
    "            new_row = group.iloc[0].copy()\n",
    "            new_row['summary'] = aug\n",
    "            new_row[label_col] = label\n",
    "            aug_rows.append(new_row)\n",
    "            needed -= 1\n",
    "            if needed <= 0: break\n",
    "        i += 1\n",
    "\n",
    "df_augmented = pd.concat([df, pd.DataFrame(aug_rows)], ignore_index=True)\n",
    "print(\"Label distribution after augmentation:\")\n",
    "print(df_augmented[label_col].value_counts())\n",
    "df_augmented.to_csv(\"data/damri_article/data_processed/data_extraction_labeled_augmented.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cca8d1",
   "metadata": {},
   "source": [
    "## Baseline Model \n",
    "TF-IDF + Naive Bayes & Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b70ba5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = df_augmented['summary']\n",
    "y = df_augmented[label_col]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "pipe_nb = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(1,2), max_features=5000)),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipe_lr = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(1,2), max_features=5000)),\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "print(\"Training Naive Bayes...\")\n",
    "pipe_nb.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training Logistic Regression...\")\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation\n",
    "y_pred_nb = pipe_nb.predict(X_test)\n",
    "y_pred_lr = pipe_lr.predict(X_test)\n",
    "\n",
    "print(\"\\n=== MultinomialNB ===\")\n",
    "print(classification_report(y_test, y_pred_nb, digits=4))\n",
    "\n",
    "print(\"\\n=== LogisticRegression ===\")\n",
    "print(classification_report(y_test, y_pred_lr, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fc09d2",
   "metadata": {},
   "source": [
    "## Check Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb60086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "labels_sorted = sorted(df_augmented[label_col].unique())\n",
    "cm = confusion_matrix(y_test, y_pred_lr, labels=labels_sorted)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=labels_sorted, yticklabels=labels_sorted)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix (Logistic Regression)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
